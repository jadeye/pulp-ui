# .gitlab-ci.yml — Security scans for a JS/TS frontend repo

include:
  - template: Jobs/SAST.gitlab-ci.yml
  - template: Jobs/Dependency-Scanning.gitlab-ci.yml
  - template: Jobs/Secret-Detection.gitlab-ci.yml

variables:
  # Set to "false" if you don't have Ultimate
  GITLAB_ADVANCED_SAST_ENABLED: "true"

  # Speed up scans by skipping noise
  SAST_EXCLUDED_PATHS: "node_modules,dist,build,coverage,.next,.cache,storybook-static,cypress,playwright-report,**/*.min.js,**/*.map"
  DS_EXCLUDED_PATHS: "node_modules,dist,build,coverage,.next,.cache,storybook-static"
  SECRET_DETECTION_EXCLUDED_PATHS: "node_modules,dist,build,coverage,.next,.cache,storybook-static"

# Only create pipelines for: MRs, default branch pushes, or schedules
workflow:
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    - if: $CI_PIPELINE_SOURCE == "schedule"
    - when: never

# SAST (Semgrep or Advanced SAST)
# Runs when JS/TS files change, on schedules, on main pushes,
# and once on any new branch for a baseline scan
semgrep-sast:
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: on_success
    - changes:
        - "**/*.js"
        - "**/*.jsx"
        - "**/*.ts"
        - "**/*.tsx"
      when: on_success
    - if: $CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH != $CI_DEFAULT_BRANCH && $CI_COMMIT_BEFORE_SHA == "0000000000000000000000000000000000000000"
      when: on_success
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
      when: on_success
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: on_success
    - when: never

# Dependency Scanning (Gemnasium)
# Runs on: schedules; when lockfiles change; on MRs; on main pushes;
# and once on any new branch (baseline) *if lockfiles exist*
gemnasium-dependency_scanning:
  variables:
    # help Gemnasium find nested lockfiles
    DS_LOCK_FILE_PATHS: "yarn.lock,package-lock.json,pnpm-lock.yaml,**/yarn.lock,**/package-lock.json,**/pnpm-lock.yaml"
  rules:
    # schedules: always
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: on_success

    # run when lockfiles change (any branch/MR)
    - changes:
        - yarn.lock
        - package-lock.json
        - pnpm-lock.yaml
      when: on_success

    # first push on a new non-default branch (baseline) if lockfiles exist (root or nested)
    - if: $CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH != $CI_DEFAULT_BRANCH && $CI_COMMIT_BEFORE_SHA == "0000000000000000000000000000000000000000"
      exists:
        - yarn.lock
        - package-lock.json
        - pnpm-lock.yaml
      when: on_success

    # all MRs
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
      when: on_success

    # pushes to default branch (main)
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: on_success

    # fallback: never
    - when: never

# Secret Detection (Gitleaks ruleset)
# Runs on: schedules, MRs, and main pushes
secret_detection:
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: on_success
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
      when: on_success
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: on_success
    - when: never

stages:
  - test
  - build
  - deploy
  - security
  - report

generate_security_report:
  stage: report
  image: alpine:3.19
  needs:
    - job: semgrep-sast
      artifacts: true
    - job: gemnasium-dependency_scanning
      artifacts: true
    - job: secret_detection
      artifacts: true
  before_script:
    - apk add --no-cache jq
    script: |
    set -e
    mkdir -p out

    md=out/security-report.md
    html=out/security-report.html

    echo "== Debug: downloaded security artifacts =="
    find . -maxdepth 3 -type f \( -name "gl-*-report.json" -o -name "gl-sbom-*.cdx.json" \) | sort || true

    SAST_JSON="${SAST_JSON:-gl-sast-report.json}"
    DS_JSON="${DS_JSON:-gl-dependency-scanning-report.json}"
    SECRETS_JSON="${SECRETS_JSON:-gl-secret-detection-report.json}"

    # Try to locate DS report if not at repo root
    DS_NOTE=""
    if [ ! -f "$DS_JSON" ]; then
      DS_JSON_FOUND=$(find . -type f -name "gl-dependency-scanning-report.json" | head -n1 || true)
      if [ -n "$DS_JSON_FOUND" ]; then
        echo "Using DS report at: $DS_JSON_FOUND"
        DS_JSON="$DS_JSON_FOUND"
        DS_NOTE="_(Using: $DS_JSON_FOUND)_"
      else
        echo "WARNING: Dependency Scanning report not found. Was the job skipped?"
      fi
    fi

    # Build Markdown
    {
      echo "# Security Report"
      echo
      echo "_Generated on $(date -u +"%Y-%m-%d %H:%M:%SZ")_"
      echo
    } > "$md"

    summarize() {
      title="$1"; file="$2"
      echo "## $title" >> "$md"
      if [ -f "$file" ]; then
        count=$(jq '.vulnerabilities | length' "$file" 2>/dev/null || echo 0)
        echo >> "$md"
        echo "**Findings:** $count" >> "$md"
        echo >> "$md"
        if [ "$count" -gt 0 ]; then
          echo "| Severity | Category | Name | File | Lines |" >> "$md"
          echo "|---|---|---|---|---|" >> "$md"
          jq -r '
            (.vulnerabilities // [])[]
            | [
                (.severity // "unknown"),
                (.category // "unknown"),
                (.name // "n/a"),
                (.location.file // "n/a"),
                (
                  ( (.location.start_line // "")|tostring )
                  + (if .location.end_line then "-" + ((.location.end_line)|tostring) else "" end)
                )
              ]
            | @tsv
          ' "$file" | awk -F'\t' '{printf("| %s | %s | %s | %s | %s |\n",$1,$2,$3,$4,$5)}' >> "$md"
        else
          echo "_No vulnerabilities detected._" >> "$md"
        fi
      else
        echo >> "$md"
        echo "_Report not found (job may have been skipped)._" >> "$md"
      fi
      echo >> "$md"
    }

    summarize "SAST" "$SAST_JSON"
    summarize "Dependency Scanning" "$DS_JSON"
    [ -n "$DS_NOTE" ] && echo "$DS_NOTE" >> "$md" && echo >> "$md"
    summarize "Secret Detection" "$SECRETS_JSON"

    # Minimal HTML wrapper
    {
      echo "<!doctype html><meta charset='utf-8'><title>Security Report</title>"
      echo "<style>body{font-family:sans-serif;max-width:980px;margin:32px auto;padding:0 16px} table{border-collapse:collapse;width:100%} th,td{border:1px solid #ddd;padding:8px} th{background:#f5f5f5;text-align:left}</style>"
      echo "<article>"
      awk '
        /^# /{print "<h1>"substr($0,3)"</h1>";next}
        /^## /{print "<h2>"substr($0,4)"</h2>";next}
        /^\|/{
          if (!intable){print "<table>"; intable=1}
          gsub(/^\||\|$/,""); n=split($0, a, /\|/)
          if (!header_done){ print "<tr>"; for(i=1;i<=n;i++) print "<th>"a[i]"</th>"; print "</tr>"; header_done=1; next }
          print "<tr>"; for(i=1;i<=n;i++) print "<td>"a[i]"</td>"; print "</tr>"; next
        }
        { if (intable){print "</table>"; intable=0; header_done=0} print "<p>"$0"</p>"}
        END{ if (intable){print "</table>"} }
      ' "$md"
      echo "</article>"
    } > "$html"

  artifacts:
    name: "security-report"
    when: always
    expire_in: 30 days
    paths:
      - out/security-report.md
      - out/security-report.html
    expose_as: "Security Report"
  rules:
    - when: always


# Publish the HTML report on GitLab Pages
pages:
  stage: report
  needs:
    - job: generate_security_report
      artifacts: true
  script:
    - mkdir -p public
    - cp out/security-report.html public/index.html
  artifacts:
    paths:
      - public
  rules:
    - when: always

publish_wikijs:
  stage: report
  image: alpine:3.19
  needs:
    - job: generate_security_report
      artifacts: true
  variables:
    WIKI_TITLE: "Pulp UI Security Report"
    WIKI_DESCRIPTION: "Security report for pulp-ui (auto-generated)"
    WIKI_TAGS: "security,report,pulp-ui"
    # WIKI_URL: http://172.16.111.195:3000     # set in CI/CD → Variables
    # WIKI_TOKEN: <your working Postman token>  # set in CI/CD → Variables (masked)
    # WIKI_LOCALE: en                           # optional; we’ll auto-detect if unset
  before_script:
    - set -eux
    - apk add --no-cache curl jq ca-certificates coreutils
    - test -f out/security-report.md
    - test -n "${WIKI_URL:-}"   || { echo "WIKI_URL required"; exit 1; }
    - test -n "${WIKI_TOKEN:-}" || { echo "WIKI_TOKEN required"; exit 1; }
    - mkdir -p wikijs-debug
    # Show exactly which URL we’ll hit (should match Postman)
    - echo "Using WIKI_URL=${WIKI_URL%/}/graphql" | tee wikijs-debug/info.txt
    # Auto-detect default locale if not provided (your Postman query worked on this path)
    - |
      if [ -z "${WIKI_LOCALE:-}" ]; then
        curl -sS -o wikijs-debug/locale.json -w '%{http_code}' \
          -H "Authorization: Bearer ${WIKI_TOKEN}" \
          -H "Content-Type: application/json" \
          --data '{"query":"{ localization { config { locale } } }"}' \
          "${WIKI_URL%/}/graphql" > wikijs-debug/locale_http.txt
        export WIKI_LOCALE=$(jq -r '.data.localization.config.locale' wikijs-debug/locale.json)
        [ -n "$WIKI_LOCALE" ] || { echo "Could not auto-detect locale"; cat wikijs-debug/locale.json; exit 1; }
      fi
    - echo "Using locale=$WIKI_LOCALE" | tee -a wikijs-debug/info.txt
  script: |
    set -euo pipefail

    PROJECT_SLUG="${CI_PROJECT_PATH_SLUG:-${CI_PROJECT_NAME:-project}}"
    TARGET_PATH="${WIKI_PATH:-security/${PROJECT_SLUG}/latest}"
    ARCHIVE_PATH="security/${PROJECT_SLUG}/$(date -u +%F)"

    MD_RAW="$(cat out/security-report.md)"
    TAGS_JSON=$(printf '%s' "${WIKI_TAGS:-}" | jq -Rs 'split(",") | map(gsub("^\\s+|\\s+$";"")) | map(select(length>0))')

    call_gql () {
      local payload="$1" out="$2"
      http=$(
        curl -sS -o "$out" -w "%{http_code}" \
          -H "Authorization: Bearer ${WIKI_TOKEN}" \
          -H "Content-Type: application/json" \
          --data "$payload" \
          "${WIKI_URL%/}/graphql"
      )
      echo "HTTP $http" >> wikijs-debug/http.log
      [ "$http" = "200" ]
    }

    # 1) Lookup page by path
    get_payload=$(jq -cn --arg path "$TARGET_PATH" --arg locale "$WIKI_LOCALE" \
      '{query:"query($path:String!,$locale:String!){ pages { singleByPath(path:$path, locale:$locale){ id path title } } }",
        variables:{path:$path, locale:$locale}}')
    echo "$get_payload" > wikijs-debug/get_payload.json
    call_gql "$get_payload" "wikijs-debug/get_result.json" || { echo "Lookup failed"; cat wikijs-debug/get_result.json; exit 1; }
    id=$(jq -r '.data.pages.singleByPath.id // empty' wikijs-debug/get_result.json)

    if [ -n "$id" ]; then
      # 2a) Update
      echo "Updating page id=$id at $TARGET_PATH"
      mut_payload=$(jq -cn --argjson id "$id" --arg content "$MD_RAW" \
        '{query:"mutation($id:Int!,$content:String!){ pages { update(id:$id, content:$content){ responseResult{ succeeded message } page{ id path } } } }",
          variables:{id:$id, content:$content}}')
      echo "$mut_payload" > wikijs-debug/update_payload.json
      call_gql "$mut_payload" "wikijs-debug/update_result.json" || { echo "Update failed"; cat wikijs-debug/update_result.json; exit 1; }
      jq -e '.errors|not and .data.pages.update.responseResult.succeeded==true' wikijs-debug/update_result.json >/dev/null \
        || { jq '.errors // .data.pages.update.responseResult' wikijs-debug/update_result.json; exit 1; }
    else
      # 2b) Create (matches your working Postman mutation)
      echo "Creating page at $TARGET_PATH"
      mut_payload=$(jq -cn \
        --arg path "$TARGET_PATH" \
        --arg title "${WIKI_TITLE}" \
        --arg locale "$WIKI_LOCALE" \
        --arg desc "${WIKI_DESCRIPTION}" \
        --arg content "$MD_RAW" \
        --argjson tags "$TAGS_JSON" \
        '{query:"mutation($path:String!,$title:String!,$locale:String!,$description:String!,$tags:[String]!, $content:String!){ pages { create(path:$path, title:$title, locale:$locale, editor:\"markdown\", isPublished:true, isPrivate:false, description:$description, tags:$tags, content:$content){ responseResult{ succeeded message } page{ id path } } } }",
          variables:{path:$path, title:$title, locale:$locale, description:$desc, tags:$tags, content:$content}}')
      echo "$mut_payload" > wikijs-debug/create_payload.json
      call_gql "$mut_payload" "wikijs-debug/create_result.json" || { echo "Create failed"; cat wikijs-debug/create_result.json; exit 1; }
      jq -e '.errors|not and .data.pages.create.responseResult.succeeded==true' wikijs-debug/create_result.json >/dev/null \
        || { jq '.errors // .data.pages.create.responseResult' wikijs-debug/create_result.json; exit 1; }
    fi

    # Optional archive
    if [ "${WIKI_ARCHIVE:-false}" = "true" ]; then
      echo "Creating archive at $ARCHIVE_PATH"
      arch_payload=$(jq -cn \
        --arg path "$ARCHIVE_PATH" \
        --arg title "${WIKI_TITLE} ($(date -u +%F))" \
        --arg locale "$WIKI_LOCALE" \
        --arg desc "${WIKI_DESCRIPTION}" \
        --arg content "$MD_RAW" \
        --argjson tags "$TAGS_JSON" \
        '{query:"mutation($path:String!,$title:String!,$locale:String!,$description:String!,$tags:[String]!, $content:String!){ pages { create(path:$path, title:$title, locale:$locale, editor:\"markdown\", isPublished:true, isPrivate:false, description:$description, tags:$tags, content:$content){ responseResult{ succeeded message } page{ id path } } } }",
          variables:{path:$path, title:$title, locale:$locale, description:$desc, tags:$tags, content:$content}}')
      echo "$arch_payload" > wikijs-debug/archive_payload.json
      call_gql "$arch_payload" "wikijs-debug/archive_result.json" || { echo "Archive create failed"; cat wikijs-debug/archive_result.json; exit 1; }
      jq -e '.errors|not and .data.pages.create.responseResult.succeeded==true' wikijs-debug/archive_result.json >/dev/null \
        || { jq '.errors // .data.pages.create.responseResult' wikijs-debug/archive_result.json; exit 1; }
    fi
  artifacts:
    when: on_failure
    paths:
      - wikijs-debug/
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    - if: $CI_PIPELINE_SOURCE == "schedule"
